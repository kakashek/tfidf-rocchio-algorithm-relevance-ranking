{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698fffaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hy/wn0xkr957cg21tmpv4xrc5dh0000gn/T/ipykernel_83343/1097072323.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcoll\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_rcv_coll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoll\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBowColl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'coll'"
     ]
    }
   ],
   "source": [
    "from coll import parse_rcv_coll\n",
    "from coll import BowColl\n",
    "import math\n",
    "import sys\n",
    "import glob, os\n",
    "import string\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "# get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# specify the file name and path relative to the cwd\n",
    "inputpath = \"DataSets\"\n",
    "file_stop_words = \"common-english-words.txt\"\n",
    "inputpath_files = os.path.join(cwd, inputpath)\n",
    "stop_words_list = os.path.join(cwd, file_stop_words)\n",
    "\n",
    "# Read the custom set of stop words from a file\n",
    "with open('common-english-words.txt', 'r') as f:\n",
    "    stop_words = set(f.read().split(','))\n",
    "\n",
    "# Define a Porter stemmer object\n",
    "def stemmer(word):\n",
    "    return stem(word)\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(inputpath_files):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.xml'):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            subfoldername = os.path.basename(dirpath)  # get the name of the subfolder\n",
    "            with open(filepath, 'r') as f:\n",
    "                contents = f.read()\n",
    "                datasets[filename] = contents\n",
    "\n",
    "N = len(datasets)\n",
    "avgdl = sum(len(dataset) for dataset in datasets.values()) / N\n",
    "\n",
    "# Define constants for BM25\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "k2 = 500\n",
    "\n",
    "# Define constants for Rocchio\n",
    "alpha = 1\n",
    "beta = 0.75\n",
    "gamma = 0.25\n",
    "\n",
    "# Read the queries from the file and store them in a dictionary\n",
    "queries = {}\n",
    "\n",
    "os.chdir(cwd)\n",
    "with open('Queries.txt', 'r') as f:\n",
    "    query_lines = f.readlines()\n",
    "    for i in range(len(query_lines)):\n",
    "        if query_lines[i].startswith('<num>'):\n",
    "            query_num = query_lines[i].strip()[6:]\n",
    "            j = i + 1\n",
    "            while not query_lines[j].startswith('<title>'):\n",
    "                j += 1\n",
    "            query_title = query_lines[j].strip()[7:]\n",
    "\n",
    "            # Tokenize the title into a list of words\n",
    "            words = query_title.split()\n",
    "\n",
    "            # Remove stop words and punctuation from the list of words\n",
    "            words = [w.lower() for w in words if w.lower() not in string.punctuation]\n",
    "\n",
    "            # Stem the remaining words using the Porter stemming algorithm\n",
    "            words = [stemmer(w) for w in words]\n",
    "\n",
    "            # Store the preprocessed query title in the dictionary\n",
    "            queries[query_num] = words\n",
    "\n",
    "# Process each query-subfolder pair\n",
    "query_nums = list(queries.keys())\n",
    "\n",
    "for i, query_num in enumerate(query_nums):\n",
    "    query_words = queries[query_num]\n",
    "    scores = {}\n",
    "\n",
    "    # Sanitize the query number for subfolder name\n",
    "    sanitized_query_num = ''.join(c for c in query_num if c.isdigit())\n",
    "\n",
    "    # Create the subfolder name based on the query number\n",
    "    subfolder_name = f\"Dataset{sanitized_query_num}\"\n",
    "\n",
    "    # Create the main folder to save the output\n",
    "    main_folder = \"Result_Model2v1\"\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    # Create the subfolder for the query\n",
    "    output_subfolder = os.path.join(main_folder, subfolder_name)\n",
    "    os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "    main_folder_path = cwd + \"/\"+ \"DataSets\"\n",
    "\n",
    "    # Process XML files in the subfolder\n",
    "    subfolder_path = os.path.join(main_folder_path, subfolder_name)\n",
    "        \n",
    "    for filename in os.listdir(subfolder_path):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            dataset_name = os.path.splitext(filename)[0]\n",
    "            dataset_path = os.path.join(subfolder_path, filename)\n",
    "\n",
    "            with open(dataset_path, 'r') as f:\n",
    "                dataset_content = f.read()\n",
    "                \n",
    "                doc_words = []\n",
    "\n",
    "                # Process document words\n",
    "                start_end = False\n",
    "                for line in dataset_content.split('\\n'):\n",
    "                    if line.startswith(\"<text>\"):\n",
    "                        start_end = True\n",
    "                    elif line.startswith(\"</text>\"):\n",
    "                        break\n",
    "                    elif start_end:\n",
    "                        # Preprocess the line\n",
    "                        line = line.replace(\"<p>\", \"\").replace(\"</p>\", \"\")\n",
    "                        line = line.translate(str.maketrans('', '', string.digits)).translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                        line = line.replace(\"\\\\s+\", \"\")\n",
    "\n",
    "                        # Tokenize the line into words\n",
    "                        words = line.split()\n",
    "\n",
    "                        # Remove stop words and punctuation from the list of words\n",
    "                        words = [w.lower() for w in words if w.lower() not in string.punctuation]\n",
    "\n",
    "                        # Stem the remaining words using the Porter stemming algorithm\n",
    "                        words = [stemmer(w) for w in words]\n",
    "\n",
    "                        # Add the preprocessed words to the document words list\n",
    "                        doc_words.extend(words)\n",
    "\n",
    "                # Calculate the idf score for each word in the query\n",
    "                idf_scores = {}\n",
    "                for word in query_words:\n",
    "                    if word not in idf_scores:\n",
    "                        df = sum(1 for filename in os.listdir(subfolder_path) if filename.endswith(\".xml\") and word in open(os.path.join(subfolder_path, filename)).read())\n",
    "                        idf = math.log10((len(os.listdir(subfolder_path)) - df + 0.5) / (df + 0.5))\n",
    "                        idf_scores[word] = idf\n",
    "\n",
    "                # Calculate the BM25 score for the query-document pair\n",
    "                K = k1 * ((1 - b) + b * len(doc_words) / avgdl)\n",
    "                score = 0\n",
    "                for word in query_words:\n",
    "                    if word not in doc_words:\n",
    "                        continue\n",
    "                    tf = doc_words.count(word)\n",
    "                    idf = idf_scores[word]\n",
    "                    qf = query_words.count(word)\n",
    "                    score += idf * ((k1 + 1) * tf / (K + tf)) * ((k2 + 1) * qf / (k2 + qf))\n",
    "\n",
    "                # Store the score for the query-document pair\n",
    "                scores[dataset_name] = score\n",
    "\n",
    "    # Apply Rocchio's algorithm for pseudo relevance feedback\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for filename in os.listdir(subfolder_path):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            dataset_name = os.path.splitext(filename)[0]\n",
    "            dataset_path = os.path.join(subfolder_path, filename)\n",
    "\n",
    "            with open(dataset_path, 'r') as f:\n",
    "                dataset_content = f.read()\n",
    "\n",
    "                doc_words = []\n",
    "\n",
    "                # Process document words\n",
    "                start_end = False\n",
    "                for line in dataset_content.split('\\n'):\n",
    "                    if line.startswith(\"<text>\"):\n",
    "                        start_end = True\n",
    "                    elif line.startswith(\"</text>\"):\n",
    "                        break\n",
    "                    elif start_end:\n",
    "                        # Preprocess the line\n",
    "                        line = line.replace(\"<p>\", \"\").replace(\"</p>\", \"\")\n",
    "                        line = line.translate(str.maketrans('', '', string.digits)).translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                        line = line.replace(\"\\\\s+\", \"\")\n",
    "\n",
    "                        # Tokenize the line into words\n",
    "                        words = line.split()\n",
    "\n",
    "                        # Remove stop words and punctuation from the list of words\n",
    "                        words = [w.lower() for w in words if w.lower() not in string.punctuation]\n",
    "\n",
    "                        # Stem the remaining words using the Porter stemming algorithm\n",
    "                        words = [stemmer(w) for w in words]\n",
    "\n",
    "                        # Add the preprocessed words to the document words list\n",
    "                        doc_words.extend(words)\n",
    "\n",
    "                # Modify the query vector with pseudo relevance feedback\n",
    "                rel_docs = [doc[0] for doc in sorted_scores[:12]]  # Get the top 12 documents\n",
    "                non_rel_docs = [doc[0] for doc in sorted_scores[12:]]  # Get the remaining documents\n",
    "\n",
    "                if dataset_name in rel_docs:\n",
    "                    rel_index = rel_docs.index(dataset_name)\n",
    "                    rel_score = sorted_scores[rel_index][1]\n",
    "                    non_rel_score = 0\n",
    "                elif dataset_name in non_rel_docs:\n",
    "                    non_rel_index = non_rel_docs.index(dataset_name)\n",
    "                    non_rel_score = sorted_scores[non_rel_index + 12][1]\n",
    "                    rel_score = 0\n",
    "\n",
    "                # Update the query vector\n",
    "                for word in query_words:\n",
    "                    if word in doc_words:\n",
    "                        tf = doc_words.count(word)\n",
    "                        idf = idf_scores[word]\n",
    "                        qf = query_words.count(word)\n",
    "\n",
    "                        # Calculate the new score with Rocchio's algorithm\n",
    "                        score = alpha * (idf * qf) + beta * (rel_score * tf / len(rel_docs)) - gamma * (non_rel_score * tf / len(non_rel_docs))\n",
    "                        scores[dataset_name] = score\n",
    "\n",
    "                        score += idf * ((k1 + 1) * tf / (K + tf)) * ((k2 + 1) * qf / (k2 + qf))\n",
    "\n",
    "    # Save the output for the query in a file\n",
    "    output_file = os.path.join(output_subfolder, \"output.txt\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        for dataset_name, score in sorted_scores:\n",
    "            f.write(f\"{dataset_name}: {score}\\n\")\n",
    "\n",
    "    print(f\"Query {query_num}: Output saved in the subfolder for the query.\")\n",
    "    print(\"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc937e97-32cd-444c-9422-60a5a02c0a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
